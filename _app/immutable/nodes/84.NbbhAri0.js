import"../chunks/CWj6FrbW.js";import"../chunks/69_IOA4Y.js";import{f as i,a as c,b as n,s as r,n as d,t as P}from"../chunks/B1HoO4k0.js";import{T as Z}from"../chunks/CcX1cm_-.js";import{T as rr}from"../chunks/CYEUtE_3.js";import{M as e}from"../chunks/e5qaD0UW.js";import{P as g}from"../chunks/B8FEnsbH.js";import{P as b}from"../chunks/ClksMsit.js";import{S as x}from"../chunks/D2QCM4hc.js";import{F as or}from"../chunks/Dq4DlXO6.js";import{H as R}from"../chunks/VyKLllFE.js";var ar=i(`<b>Энтропия</b>&nbsp;&mdash; мера неопределенности или мера хаоса какой-то системы.
        Обозначается буквой <!>.`,1),er=i("<!> <!>",1),nr=i("Можно определить <b>энтропию Шеннона</b> как",1),tr=i("Для непрерывного случая с носителем <!> и плотностью распределения <!>",1),dr=i("<!> <!> <!> <!>",1),_r=i(`Посмотрим на&nbsp;<!> испытаний, среди которых <!> удачных
        и&nbsp;<!> неудачных. Результаты испытаний для
        известных <!> и <!> кодируются последовательностью бит.
        Всего таких последовательностей <!>.
        Значит, для кодирования каждой последовательности достаточно <!> бит.`,1),vr=i(`Получается, что в среднем для кодирования результатов одного исхода
        понадобится в среднем <!> бит.
        По&nbsp;формуле Стирлинга <!>,`,1),lr=i("<!> <!> <!> <!>",1),sr=i(`Например, у системы &laquo;брошенная монетка&raquo; с распределением
        вероятностей <!>, энтропия`,1),mr=i(`А у системы &laquo;брошенная ненормальная монетка&raquo; с распределением
        вероятностей <!> энтропия`,1),pr=i("<!> <!> <!> <!> <!>",1),ir=i("<!> <!>",1),cr=i("Энтропия системы с <!> состояниями ограничена числом <!>.",1),$r=i("<!> <!>",1),gr=i("Если <!> и <!>&nbsp;&mdash; независимые системы, то <!>.",1),fr=i("Энтропия равномерного распределения <!>",1),hr=i("<!> <!>",1),Pr=i(`Энтропия нормального распределения <!> с
        плотностью <!>`,1),br=i("<!> <!>",1),xr=i(`Энтропия показательного распределения <!> с
        плотностью <!>`,1),ur=i("<!> <!>",1),qr=i(`Пусть у нас есть две системы <!> и <!>.
        Как понять, насколько <!> отличается от <!>?`,1),Hr=i(`<b>Кросс-энтропия</b> служит показателем информации, необходимой для распознания одного исхода, если схема
        кодирования базируется не на истинном распределении <!>, а на другом распределении <!>.`,1),yr=i("<!> <!> <!>",1),Xr=i(`Если распределение <!> близко к распределению <!>, то кросс-энтропия <!> близка к обычной энтропии <!>, а полное совпадение <!> происходит в
        случае, когда распределения <!> и <!> совпадают почти всюду.`,1),Tr=i(`<b>Расстоянием Кульбака-Лейблера</b> между двумя распределениями <!> и <!> называется
        величина`,1),Or=i(`Расстояние Кульбака-Лейблера говорит об увеличении среднего количества информации, если при кодировании
        использовать распределение <!> вместо истинного распределения <!>.`,1),Yr=i("<!> <!> <!> <!>",1),Fr=i("<!> <!>",1),Mr=i("<!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!> <!>",1);function Cr(U){var u=Mr(),q=c(u);Z(q,{title:"Энтропия"});var H=r(q,2);b(H,{children:(t,f)=>{g(t,{children:(o,v)=>{d();var l=P(`Речь пойдет преимущественно про разные системы. С точки зрения вероятностной интерпретации происходящего, можно
        рассматривать систему как совокупность состояний, каждое из которых может реализовываться с какой-то
        вероятностью. Тогда событие в этом вероятностном пространстве представляют собой реализацию какого-то состояния
        из множества.`);n(o,l)}})}});var y=r(H,2);R(y,{children:(t,f)=>{d();var o=P("Энтропия");n(t,o)}});var X=r(y,2);b(X,{children:(t,f)=>{var o=er(),v=c(o);g(v,{children:(_,p)=>{var a=ar(),s=r(c(a),2);e(s,{m:"\\H"}),d(),n(_,a)}});var l=r(v,2);g(l,{children:(_,p)=>{d();var a=P(`Чем больше неопределенность системы, тем больше нужно информации для кодирования её состояния. Получается, что
        энтропия служит еще и мерой информации, необходимой для описания состояния системы.`);n(_,a)}}),n(t,o)}});var T=r(X,2);b(T,{children:(t,f)=>{var o=dr(),v=c(o);g(v,{children:(a,s)=>{d();var m=nr();d(2),n(a,m)}});var l=r(v,2);e(l,{display:!0,m:"\\H(\\Omega) \\defeq - \\sum_{\\omega \\in \\Omega} \\prob(\\omega) \\cdot \\log \\prob(\\omega)"});var _=r(l,2);g(_,{children:(a,s)=>{d();var m=tr(),$=r(c(m));e($,{m:"X"});var h=r($,2);e(h,{m:"f(x)"}),n(a,m)}});var p=r(_,2);e(p,{display:!0,m:"\\H \\defeq - \\int_{X} f(x) \\cdot \\log f(x) dx"}),n(t,o)}});var O=r(T,2);or(O,{title:"Откуда взялась эта формула?",children:(t,f)=>{var o=lr(),v=c(o);g(v,{children:(a,s)=>{d();var m=P(`Давайте попробуем оценить количество информации,
        необходимое для кодирования одного исхода.`);n(a,m)}});var l=r(v,2);g(l,{children:(a,s)=>{d();var m=_r(),$=r(c(m));e($,{m:"n"});var h=r($,2);e(h,{m:"a \\approx pn"});var G=r(h,2);e(G,{m:"b \\approx (1-p) \\cdot n"});var I=r(G,2);e(I,{m:"a"});var J=r(I,2);e(J,{m:"b"});var Q=r(J,2);e(Q,{m:"\\binom{n}{a} = \\frac{n!}{a! \\cdot b!}"});var W=r(Q,2);e(W,{m:"\\log \\frac{n!}{a! \\cdot b!}"}),d(),n(a,m)}});var _=r(l,2);g(_,{children:(a,s)=>{d();var m=vr(),$=r(c(m));e($,{m:"\\frac{1}{n} \\log \\frac{n!}{a! \\cdot b!}"});var h=r($,2);e(h,{m:"\\log n! = n \\log n - n + O(\\log n)"}),d(),n(a,m)}});var p=r(_,2);e(p,{display:!0,m:`\\frac{1}{n} \\cdot \\log \\frac{n!}{a! \\cdot b!} =
                      \\frac{1}{n} \\cdot \\big(\\log n! - \\log a! - \\log b!\\big) \\approx
                      - p \\log p - (1-p) \\cdot \\log (1-p)`}),n(t,o)}});var Y=r(O,2);b(Y,{children:(t,f)=>{var o=pr(),v=c(o);g(v,{children:(s,m)=>{d();var $=sr(),h=r(c($));e(h,{m:"\\big(\\frac{1}{2}, \\frac{1}{2}\\big)"}),d(),n(s,$)}});var l=r(v,2);e(l,{display:!0,m:"\\H = 2 \\cdot \\frac{1}{2} \\cdot \\log_2 2 = 1"});var _=r(l,2);g(_,{children:(s,m)=>{d();var $=mr(),h=r(c($));e(h,{m:"\\big(\\frac{1}{4}, \\frac{3}{4}\\big)"}),d(),n(s,$)}});var p=r(_,2);e(p,{display:!0,m:"\\H = \\frac{1}{4} \\cdot \\log_2 4 + \\frac{3}{4} \\cdot \\log_2 \\frac{4}{3} \\approx 0.56"});var a=r(p,2);g(a,{children:(s,m)=>{d();var $=P(`Энтропия нормальной монетки больше энтропии ненормальной. Значит, сообщение о результате броска нормальной
        монетки несет больше информации, чем сообщение о результате броска ненормальной.`);n(s,$)}}),n(t,o)}});var F=r(Y,2);x(F,{children:(t,f)=>{d();var o=P("Свойства энтропии");n(t,o)}});var M=r(F,2);b(M,{children:(t,f)=>{var o=ir(),v=c(o);g(v,{children:(_,p)=>{d();var a=P("Энтропия системы равна нулю только тогда, когда система состоит из одного состояния.");n(_,a)}});var l=r(v,2);g(l,{children:(_,p)=>{d();var a=P("Действительно, сообщение о состоянии такой системы не несет вообще никакой информации. Мы и так это знали.");n(_,a)}}),n(t,o)}});var S=r(M,2);b(S,{children:(t,f)=>{var o=$r(),v=c(o);g(v,{children:(_,p)=>{d();var a=cr(),s=r(c(a));e(s,{m:"n"});var m=r(s,2);e(m,{m:"\\log n"}),d(),n(_,a)}});var l=r(v,2);e(l,{display:!0,m:"\\H = \\sum_{i=1}^n p_i \\log \\frac{1}{p_i} \\le \\log \\bigg( \\sum_{i=1}^n p_i \\cdot \\frac{1}{p_i} \\bigg) = \\log n"}),n(t,o)}});var E=r(S,2);b(E,{children:(t,f)=>{g(t,{children:(o,v)=>{d();var l=gr(),_=r(c(l));e(_,{m:"X"});var p=r(_,2);e(p,{m:"Y"});var a=r(p,2);e(a,{m:"\\H(X \\cdot Y) = \\H(X) + \\H(Y)"}),d(),n(o,l)}})}});var K=r(E,2);x(K,{children:(t,f)=>{d();var o=P("Примеры распределений");n(t,o)}});var L=r(K,2);b(L,{children:(t,f)=>{var o=hr(),v=c(o);g(v,{children:(_,p)=>{d();var a=fr(),s=r(c(a));e(s,{m:"\\uniform [a, b]"}),n(_,a)}});var l=r(v,2);e(l,{display:!0,m:"\\H = \\int_a^b \\frac{1}{b-a} \\cdot \\log (b-a) dx = \\log (b-a)"}),n(t,o)}});var N=r(L,2);b(N,{children:(t,f)=>{var o=br(),v=c(o);g(v,{children:(_,p)=>{d();var a=Pr(),s=r(c(a));e(s,{m:"\\N (\\mu, \\sigma^2)"});var m=r(s,2);e(m,{m:"f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}"}),n(_,a)}});var l=r(v,2);e(l,{display:!0,m:`\\H =
                      - \\int_{-\\infty}^{\\infty}
                        \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\cdot
                        \\log \\bigg( \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\bigg) dx =
                      \\frac{1}{2} \\log (2 \\pi \\sigma^2) + \\frac{1}{2}`}),n(t,o)}});var j=r(N,2);b(j,{children:(t,f)=>{var o=ur(),v=c(o);g(v,{children:(_,p)=>{d();var a=xr(),s=r(c(a));e(s,{m:"\\Exp (\\lambda)"});var m=r(s,2);e(m,{m:"f(x) = \\lambda e^{-\\lambda x}"}),n(_,a)}});var l=r(v,2);e(l,{display:!0,m:`\\H = - \\int_{0}^{\\infty}
                              \\lambda e^{-\\lambda x} \\cdot \\log \\big( \\lambda e^{-\\lambda x} \\big) dx =
                              1 - \\log \\lambda`}),n(t,o)}});var k=r(j,2);R(k,{children:(t,f)=>{d();var o=P("Расстояние Кульбака-Лейблера и кросс-энтропия");n(t,o)}});var w=r(k,2);b(w,{children:(t,f)=>{g(t,{children:(o,v)=>{d();var l=qr(),_=r(c(l));e(_,{m:"p"});var p=r(_,2);e(p,{m:"q"});var a=r(p,2);e(a,{m:"q"});var s=r(a,2);e(s,{m:"p"}),d(),n(o,l)}})}});var z=r(w,2);x(z,{children:(t,f)=>{d();var o=P("Кросс-энтропия");n(t,o)}});var A=r(z,2);b(A,{children:(t,f)=>{var o=yr(),v=c(o);g(v,{children:(p,a)=>{var s=Hr(),m=r(c(s),2);e(m,{m:"p"});var $=r(m,2);e($,{m:"q"}),d(),n(p,s)}});var l=r(v,2);g(l,{children:(p,a)=>{d();var s=P("Эту сложную фразу можно записать как");n(p,s)}});var _=r(l,2);e(_,{display:!0,m:"\\H(p, q) \\defeq - \\sum_x p(x) \\cdot \\log q(x) = - \\int_X p(x) \\cdot \\log q(x) dx"}),n(t,o)}});var B=r(A,2);b(B,{children:(t,f)=>{g(t,{children:(o,v)=>{d();var l=Xr(),_=r(c(l));e(_,{m:"q"});var p=r(_,2);e(p,{m:"p"});var a=r(p,2);e(a,{m:"\\H(p, q)"});var s=r(a,2);e(s,{m:"\\H(p)"});var m=r(s,2);e(m,{m:"\\H(p, q) = \\H(p)"});var $=r(m,2);e($,{m:"p"});var h=r($,2);e(h,{m:"q"}),d(),n(o,l)}})}});var C=r(B,2);x(C,{children:(t,f)=>{d();var o=P("Расстояние Кульбака-Лейблера");n(t,o)}});var D=r(C,2);b(D,{children:(t,f)=>{var o=Yr(),v=c(o);g(v,{children:(a,s)=>{d();var m=P("Теперь мы можем измерять степень различия двух распределений.");n(a,m)}});var l=r(v,2);g(l,{children:(a,s)=>{var m=Tr(),$=r(c(m),2);e($,{m:"p"});var h=r($,2);e(h,{m:"q"}),d(),n(a,m)}});var _=r(l,2);e(_,{display:!0,m:"\\KL(p, q) \\defeq \\H(p, q) - \\H(p) = - \\sum_x p(x) \\cdot \\log \\frac{p(x)}{q(x)}"});var p=r(_,2);g(p,{children:(a,s)=>{d();var m=Or(),$=r(c(m));e($,{m:"q"});var h=r($,2);e(h,{m:"p"}),d(),n(a,m)}}),n(t,o)}});var V=r(D,2);rr(V,{title:"Принцип максимальной энтропии",children:(t,f)=>{var o=Fr(),v=c(o);g(v,{children:(_,p)=>{d();var a=P("Среди всех распределений на заданном носителе мы хотим иметь дело с имеющим наибольшую энтропию.");n(_,a)}});var l=r(v,2);g(l,{children:(_,p)=>{d();var a=P("Довольно естественно: чем больше энтропия, тем более «произвольное» у нас распределение.");n(_,a)}}),n(t,o)}}),n(U,u)}export{Cr as component};
